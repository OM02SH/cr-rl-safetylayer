{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import commonroad_rl.gym_commonroad\n",
    "\n",
    "# kwargs overwrites configs defined in commonroad_rl/gym_commonroad/configs.yaml\n",
    "env = gym.make(\"commonroad-v1\",\n",
    "\t\taction_configs={\"action_type\": \"continuous\"},\n",
    "               goal_configs={\"observe_distance_goal_long\": True, \"observe_distance_goal_lat\": True},\n",
    "               surrounding_configs={\"observe_lane_circ_surrounding\": False,\n",
    "               \t\t     \"fast_distance_calculation\": False,\n",
    "                                    \"observe_lidar_circle_surrounding\": True,\n",
    "                                    \"lidar_circle_num_beams\": 20},\n",
    "               reward_type=\"sparse_reward\",\n",
    "               reward_configs={\"sparse_reward\":{\"reward_goal_reached\": 50.,\n",
    "                                      \"reward_collision\": -100.,\n",
    "                                      \"reward_off_road\": -50.,\n",
    "      \t\t\t\t\t\"reward_time_out\": -10.,\n",
    "\t\t\t\t\t\"reward_friction_violation\": 0.}})\n",
    "\n",
    "observation = env.reset()\n",
    "for i in range(50):\n",
    "    # env.render() # rendered images with be saved under ./img\n",
    "    action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "    observation, reward, terminated, done, info = env.step(action)\n",
    "    # print(\"LEN: \", len(observation))\n",
    "    # print(i, observation, reward, terminated, done, info)\n",
    "    print(info[\"detected_obstacles\"])\n",
    "    if terminated:\n",
    "        print(\"Terminated at i: \", i)\n",
    "        observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import commonroad_rl.gym_commonroad\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CallbackList, BaseCallback\n",
    "from tqdm import trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# kwargs overwrites configs defined in commonroad_rl/gym_commonroad/configs.yaml\n",
    "env = gym.make(\"commonroad-v1\",\n",
    "\t\taction_configs={\"action_type\": \"continuous\"},\n",
    "               goal_configs={\"observe_distance_goal_long\": True, \"observe_distance_goal_lat\": True},\n",
    "               surrounding_configs={\"observe_lane_circ_surrounding\": False,\n",
    "               \t\t     \"fast_distance_calculation\": False,\n",
    "                                    \"observe_lidar_circle_surrounding\": True,\n",
    "                                    \"lidar_circle_num_beams\": 20},\n",
    "               reward_type=\"sparse_reward\",\n",
    "               reward_configs={\"sparse_reward\":{\"reward_goal_reached\": 50.,\n",
    "                                      \"reward_collision\": -100.,\n",
    "                                      \"reward_off_road\": -50.,\n",
    "      \t\t\t\t\t\"reward_time_out\": -10.,\n",
    "\t\t\t\t\t\"reward_friction_violation\": 0.}})\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./ppo_commonroad_tensorboard/\")\n",
    "\n",
    "# 添加一个评估回调\n",
    "eval_env = gym.make(\"commonroad-v1\",\n",
    "\t\t            action_configs={\"action_type\": \"continuous\"},\n",
    "                    goal_configs={\"observe_distance_goal_long\": True, \"observe_distance_goal_lat\": True},\n",
    "                    surrounding_configs={\"observe_lane_circ_surrounding\": False,\n",
    "               \t\t                \"fast_distance_calculation\": False,\n",
    "                                    \"observe_lidar_circle_surrounding\": True,\n",
    "                                    \"lidar_circle_num_beams\": 20},\n",
    "                    reward_type=\"sparse_reward\",\n",
    "                    reward_configs={\"sparse_reward\":{\"reward_goal_reached\": 50.,\n",
    "                                    \"reward_collision\": -100.,\n",
    "                                    \"reward_off_road\": -50.,\n",
    "      \t\t\t\t\t            \"reward_time_out\": -10.,\n",
    "\t\t\t\t\t                \"reward_friction_violation\": 0.}})\n",
    "\n",
    "\n",
    "class TqdmEvalCallback(BaseCallback):\n",
    "    def __init__(self, eval_env, eval_freq=1000, n_eval_episodes=5, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_freq = eval_freq\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            rewards = []\n",
    "            lengths = []\n",
    "            for ep in trange(self.n_eval_episodes, desc=f\"[Eval] @ Step {self.num_timesteps}\", leave=False):\n",
    "                obs, _ = self.eval_env.reset()\n",
    "                done = False\n",
    "                total_reward = 0.0\n",
    "                ep_len = 0\n",
    "                while not done:\n",
    "                    action, _ = self.model.predict(obs, deterministic=True)\n",
    "                    obs, reward, terminated, truncated, _ = self.eval_env.step(action)\n",
    "                    done = terminated or truncated\n",
    "                    total_reward += reward\n",
    "                    ep_len += 1\n",
    "                rewards.append(total_reward)\n",
    "                lengths.append(ep_len)\n",
    "            mean_reward = sum(rewards) / len(rewards)\n",
    "            mean_length = sum(lengths) / len(lengths)\n",
    "            print(f\"\\n Evaluation at step {self.num_timesteps}:\")\n",
    "            print(f\"   ➤ Mean reward: {mean_reward:.2f} | Mean length: {mean_length:.2f}\")\n",
    "        return True\n",
    "\n",
    "eval_callback = TqdmEvalCallback(eval_env=eval_env, eval_freq=10, n_eval_episodes=5)\n",
    "\n",
    "callback = CallbackList([eval_callback])\n",
    "\n",
    "# 训练\n",
    "model.learn(total_timesteps=100, callback=callback, progress_bar=True)\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"ppo_commonroad_policy\")\n",
    "\n",
    "# 关闭环境\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "commonroad-py39-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
